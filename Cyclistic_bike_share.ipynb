{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ty3HDWhdljrV"
   },
   "source": [
    "# Cyclistic bike-share analysis case study!\n",
    "**Domain:Business Decisions**\n",
    "\n",
    "**Context:**\n",
    "\n",
    "You are a junior data analyst working in the marketing analyst team at Cyclistic, a bike-share company in Chicago. The director\n",
    "of marketing believes the company’s future success depends on maximizing the number of annual memberships. Therefore,\n",
    "your team wants to understand how casual riders and annual members use Cyclistic bikes differently. From these insights,\n",
    "your team will design a new marketing strategy to convert casual riders into annual members. But first, Cyclistic executives\n",
    "must approve your recommendations, so they must be backed up with compelling data insights and professional data\n",
    "visualizations.\n",
    "\n",
    "**Stakeholder:**\n",
    "* ****Cyclistic:**** A bike-share program that features more than 5,800 bicycles and 600 docking stations. Cyclistic sets itself\n",
    "apart by also offering reclining bikes, hand tricycles, and cargo bikes, making bike-share more inclusive to people with\n",
    "disabilities and riders who can’t use a standard two-wheeled bike. The majority of riders opt for traditional bikes; about\n",
    "8% of riders use the assistive options. Cyclistic users are more likely to ride for leisure, but about 30% use them to\n",
    "commute to work each day.\n",
    "* ****Lily Moreno:**** The director of marketing and your manager. Moreno is responsible for the development of campaigns\n",
    "and initiatives to promote the bike-share program. These may include email, social media, and other channels.\n",
    "* ****Cyclistic marketing analytics team:**** A team of data analysts who are responsible for collecting, analyzing, and\n",
    "reporting data that helps guide Cyclistic marketing strategy. You joined this team six months ago and have been busy\n",
    "learning about Cyclistic’s mission and business goals — as well as how you, as a junior data analyst, can help Cyclistic\n",
    "achieve them.\n",
    "* ****Cyclistic executive team:**** The notoriously detail-oriented executive team will decide whether to approve the\n",
    "recommended marketing program.\n",
    "\n",
    "**Data:**\n",
    "yearly_data.csv:A dataset containing information about riders who have use cyclist bike service from May-2022 to April - 2023\n",
    "\n",
    "**Data Dictionary:**\n",
    "-ride_id:It is uniuqe value assign for each ride.\n",
    "-rideable_type: It is type of bike use while going for ride.\n",
    "-started_at: It is timestamp of when the ride was started.\n",
    "-ended_at: It is timestamp of when the ride was ended.\n",
    "-start_station_name: It is the start station from where the bike is been collected for ride.\n",
    "-start_station_id: It is start station  unique id.\t\n",
    "-end_station_name: It is the destination station from where the bike is been left after ride.\t\n",
    "-end_station_id: It is end station unique id.\n",
    "-start_lat: Start station location wise latitude.\t\n",
    "-start_lng: Start station location wise longitude.\t\n",
    "-end_lat: End station location wise latitude.\t\n",
    "-end_lng: End station location wise longitude.\t\n",
    "-member_casual: It specify the ride is taken by a new customer or member customer.\n",
    "\n",
    "\n",
    "**Project Objective**\n",
    "\n",
    "Moreno has set a clear goal: Design marketing strategies aimed at converting casual riders into annual members. In order to\n",
    "do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why\n",
    "casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are\n",
    "interested in analyzing the Cyclistic historical bike trip data to identify trends.\n",
    "\n",
    "**Moreno has assigned you the first question to answer:** How do annual members and casual riders use Cyclistic bikes\n",
    "differently?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VYlrZirqAc-"
   },
   "source": [
    "**Key learning after this project:** \n",
    "\n",
    "- Data cleaning is the process of identifying and correcting or removing errors, inconsistencies, and inaccuracies in a dataset.\n",
    "- Observation writing involves examining the data and noting any notable findings, anomalies, or areas of interest.\n",
    "- Exploratory Data Analysis (EDA) is the process of examining and visualizing a dataset to understand its main characteristics, such as the distribution of data, the relationships between variables, and any anomalies or patterns that may exist. The goal of EDA is to uncover insights and trends that can help inform further analysis or decision-making. It is often the first step in any data analysis project, as it provides a foundation for more advanced statistical methods and models.\n",
    "- Visualization using matplot lib and seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "NZ2D5mYBqDc9"
   },
   "outputs": [],
   "source": [
    "#Import Libraries for Data Analysis\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every month data is stored in different csv so, we combine the data to create a Yearly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "cA0qNIwCqL6m",
    "outputId": "62b5cb66-1a76-4c35-c89c-1d6e85cd531c"
   },
   "outputs": [],
   "source": [
    "# Get CSV file into python\n",
    "\n",
    "may_2022 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202205-divvy-tripdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "april_2023 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202304-divvy-tripdata.csv')\n",
    "june_2022 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202206-divvy-tripdata.csv')\n",
    "july_2022 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202207-divvy-tripdata.csv')\n",
    "aug_2022 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202208-divvy-tripdata.csv')\n",
    "sept_2022 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202209-divvy-tripdata.csv')\n",
    "oct_2022 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202210-divvy-tripdata.csv')\n",
    "nov_2022 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202211-divvy-tripdata.csv')\n",
    "dec_2022 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202212-divvy-tripdata.csv')\n",
    "jan_2023 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202301-divvy-tripdata.csv')\n",
    "feb_2023 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202302-divvy-tripdata.csv')\n",
    "march_2023 = pd.read_csv(r'C:\\Users\\a886809\\OneDrive - Atos\\Desktop\\Google Data Analysis\\202303-divvy-tripdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before merging the table lets check rows and columns from table so its matches with yearly table to see if no data is been erased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(may_2022.shape)\n",
    "print(june_2022.shape)\n",
    "print(july_2022.shape)\n",
    "print(aug_2022.shape)\n",
    "print(sept_2022.shape)\n",
    "print(oct_2022.shape)\n",
    "print(nov_2022.shape)\n",
    "print(dec_2022.shape)\n",
    "print(jan_2023.shape)\n",
    "print(feb_2023.shape)\n",
    "print(march_2023.shape)\n",
    "print(april_2023.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows=634858+769204+823488+785932+701339+558685+337735+181806+190301+190445+258678+426590\n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all table into single table\n",
    "\n",
    "yearly_data = pd.concat([may_2022,june_2022,july_2022,aug_2022,sept_2022,oct_2022,nov_2022,dec_2022,jan_2023,feb_2023,march_2023,april_2023], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(yearly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compare rows for data deletion\n",
    "print(yearly_data.shape)\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging of all csv file we can find same number of rows and column. We can say that no data was erased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get more info about Datatypes and null values in Data\n",
    "\n",
    "yearly_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values for each column\n",
    "print(yearly_data['ride_id'].isnull().any())\n",
    "print(yearly_data['rideable_type'].isnull().any())\n",
    "print(yearly_data['started_at'].isnull().any())\n",
    "print(yearly_data['ended_at'].isnull().any())\n",
    "print(yearly_data['start_station_name'].isnull().any())\n",
    "print(yearly_data['start_station_id'].isnull().any())\n",
    "print(yearly_data['end_station_name'].isnull().any())\n",
    "print(yearly_data['end_station_id'].isnull().any())\n",
    "print(yearly_data['start_lat'].isnull().any())\n",
    "print(yearly_data['start_lng'].isnull().any())\n",
    "print(yearly_data['end_lat'].isnull().any())\n",
    "print(yearly_data['end_lng'].isnull().any())\n",
    "print(yearly_data['member_casual'].isnull().any())\n",
    "# In this code, the isnull() method is called on the \"Winners\" column of the DataFrame.\n",
    "# isnull() returns a boolean Series where each element indicates whether the corresponding value in the column is null (True) or not (False). \n",
    "# The any() method is then used to check if there is at least one True value in the Series, indicating the presence of null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are null values in start and end station id and name.Also end lng and lat have null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create a new dataset with columns which is useful for our business task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=yearly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop not required column\n",
    "df.drop(['start_station_name','start_station_id','end_station_name','end_station_id','start_lat','start_lng','end_lat','end_lng'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to seprate date and time of stared_at and ended_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column to datetime format\n",
    "df['started_at_time']=pd.to_datetime(df['started_at'],errors = 'coerce')\n",
    "\n",
    "# convert the column to HH:MM:SS format\n",
    "df['started_at_time']=df['started_at_time'].dt.strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column to datetime format\n",
    "df['ended_at_time']=pd.to_datetime(df['ended_at'],errors = 'coerce')\n",
    "\n",
    "# convert the column to HH:MM:SS format\n",
    "df['ended_at_time']=df['ended_at_time'].dt.strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column to datetime format\n",
    "df['started_at_date']=pd.to_datetime(df['started_at'],errors = 'coerce')\n",
    "\n",
    "# convert the column to YY:MM:DD format\n",
    "df['started_at_date']=df['started_at_date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column to datetime format\n",
    "df['ended_at_date']=pd.to_datetime(df['ended_at'],errors = 'coerce')\n",
    "\n",
    "# convert the column to YY:MM:DD format\n",
    "df['ended_at_date']=df['ended_at_date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the ride length by using start_point and end_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "format='%H:%M:%S'\n",
    "df['ride_length']=pd.to_datetime(df['ended_at_time'],errors = 'coerce',format='%H:%M:%S')-pd.to_datetime(df['started_at_time'],errors = 'coerce',format='%H:%M:%S')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the new created ride_length column contains null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ride_length'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a new column weekday to know each day of rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get day name in weekdays column\n",
    "df['weekday'] = pd.to_datetime(df['started_at_date']).dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ride_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out that there are some rows which have negative days which means that data recorded is not accurate.We have remove them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"ride_length\"]<= '0 days 00:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 30141 rows with start time greater than end time which means they are recorded wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We reomve rows which have negative time.\n",
    "df=df[df[\"ride_length\"]> '0 days 00:00:00'].reset_index(drop=True)\n",
    "\n",
    "#.reset_index will reset the index from 0,1,2....\n",
    "#drop=True will drop current index and reset it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataset contains 5828920 rows and 11 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis purpose lets create another column from ride_length which will convert the ride_length columns into minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ended_at_time']=pd.to_datetime(df['ended_at_time'],errors = 'coerce',format='%H:%M:%S')\n",
    "df['started_at_time']=pd.to_datetime(df['started_at_time'],errors = 'coerce',format='%H:%M:%S')\n",
    "df['ride_length2']=df['ended_at_time']-df['started_at_time']\n",
    "#Convert into minutes\n",
    "df['ride_length2']=(df['ride_length2'].dt.total_seconds() // 60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['started_at_time']=pd.to_datetime(df['started_at_time'],errors = 'coerce')\n",
    "\n",
    "# convert the column to HH:MM:SS format\n",
    "df['started_at_time']=df['started_at_time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "df['ended_at_time']=pd.to_datetime(df['ended_at_time'],errors = 'coerce')\n",
    "\n",
    "# convert the column to HH:MM:SS format\n",
    "df['ended_at_time']=df['ended_at_time'].dt.strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are customer which have used bike for minimum ride length on just 1 second and there are customer which have used bike to ride for almost day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets visualize dataset to know bike type ride lengths.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum the total minutes from each each rideable_type\n",
    "total_minutes = df.groupby(['rideable_type'])['ride_length2'].sum()\n",
    "\n",
    "# Create the bar chart\n",
    "total_minutes.plot(kind='bar')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Bike type')\n",
    "plt.ylabel('Total Minutes ride_length')\n",
    "plt.title('Bike Type wise ride lengths')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: 1e7 is nx10^7***\n",
    "\n",
    "From this bar chart we can visualize that classic Bikes have driven the most by customer and docked bike has been driven the least number of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out bike tpye wise days for each weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum the total frequecy of weekday from each each rideable_type\n",
    "count_weekday = df.groupby(['weekday','rideable_type']).size().unstack()\n",
    "\n",
    "# Create the bar chart\n",
    "count_weekday.plot(kind='bar', stacked=True)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('weekdays')\n",
    "plt.ylabel('count_days')\n",
    "plt.title('days wise count for each bike type')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that saturday is the busiest weekday for customer to ride bike amd monday is less busiest day. We can also see that there is tough competition between classic bike and electric bike for the most used for each weekday.\n",
    "\n",
    "Lets deep dive into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classic = df[df['rideable_type'] == \"classic_bike\"]\n",
    "df_electric = df[df['rideable_type'] == \"electric_bike\"]\n",
    "#sum the total minutes from each each rideable_type\n",
    "count_weekday = df_classic.groupby(['weekday']).size()\n",
    "count_weekday_electric = df_electric.groupby(['weekday']).size()\n",
    "# Create the bar chart\n",
    "count_weekday.plot( kind='bar')\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Classic bike')\n",
    "plt.ylabel('Total ride per weekdays')\n",
    "plt.title('Classic bike ride per weekdays ')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n",
    "\n",
    "count_weekday_electric.plot( kind='bar',color='green')\n",
    "# Add labels and title\n",
    "plt.xlabel('Electric bike')\n",
    "plt.ylabel('Total ride per weekdays')\n",
    "plt.title('Electric bike ride per weekdays')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say after observation electric bike have on Average 400000 rides per weekdays and they are the most used bike on every weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do annual members and casual riders use Cyclistic bikes differently?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum the total frequecy of weekday from each each rideable_type\n",
    "count_ride = df.groupby(['member_casual','rideable_type']).size()\n",
    "\n",
    "stacked_data =count_ride.unstack()\n",
    "# Create the bar chart\n",
    "ax=stacked_data.plot(kind='bar', stacked=True)\n",
    "\n",
    "# Add data labels to the bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center', color='white', fontsize=8, weight='bold')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Customer Type')\n",
    "plt.ylabel('count_rideable_type_ride')\n",
    "plt.title('Bar Chart')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this stacked bar chart that casual customer used electric_bike the most and they also used docked_bike which is not used by member customer. Member customer on other side used electric and classic bike equally approximately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate average ride length for each rideable type on members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe for average ride length for member\n",
    "member_df=df[df['member_casual']=='member']\n",
    "avg_ride_length_member = member_df.pivot_table(values='ride_length2', index=('member_casual'), columns='weekday', aggfunc='mean')\n",
    "\n",
    "\n",
    "#Create new dataframe for average ride length for casual\n",
    "casual_df=df[df['member_casual']=='casual']\n",
    "avg_ride_length_casual = casual_df.pivot_table(values='ride_length2', index=('member_casual'), columns='weekday', aggfunc='mean')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(avg_ride_length_member, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('weekdays')\n",
    "plt.ylabel('avg_ride_length')\n",
    "plt.title('Avearge ride length per weekday for member')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(avg_ride_length_casual, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('weekday')\n",
    "plt.ylabel('avg_ride_length')\n",
    "plt.title('Avearge ride length per weekday for causal')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this heat map that both customer type travel the most on saturday and sunday.\n",
    "We can see that casual customer also travel more on monday and friday than memeber customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at trip taken by each member and causal customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_df=df[df['member_casual']=='member']\n",
    "no_of_trips_member = member_df.pivot_table(values='ride_id', index=('member_casual'), columns='weekday', aggfunc='count')\n",
    "\n",
    "casual_df=df[df['member_casual']=='casual']\n",
    "no_of_trips_casual = casual_df.pivot_table(values='ride_id', index=('member_casual'), columns='weekday', aggfunc='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(no_of_trips_member, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('weekday')\n",
    "plt.ylabel('no_of_trips')\n",
    "plt.title('Bar Chart')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(no_of_trips_casual, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('weekday')\n",
    "plt.ylabel('no_of_trips')\n",
    "plt.title('Bar Chart')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this heat maps we can analyze that member customer take bike trips on weekdays more on other side casual customer take more trips on saturday and sunday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see which cateogry (member or causual) take ride for long distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_ride_length = df.pivot_table(values='ride_length2', index=('member_casual'), aggfunc='sum')\n",
    "avg_ride_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_trips = df.pivot_table(values='ride_id', index=('member_casual'), aggfunc='count')\n",
    "no_of_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=avg_ride_length.iloc[0,0]/no_of_trips.iloc[0,0]\n",
    "print(f'ave_ride_length_casual:{avg_ride_length.iloc[0,0]/no_of_trips.iloc[0,0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=avg_ride_length.iloc[1,0]/no_of_trips.iloc[1,0]\n",
    "print(f'ave_ride_length_member:{avg_ride_length.iloc[1,0]/no_of_trips.iloc[1,0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[a,b]\n",
    "x = ['causual','member']\n",
    "plt.bar(x,data)\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Bar Chart')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly visualize that causal take more long distance ride than members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to also seen that docked bike is use by causal customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docked_bike_df=df[df['rideable_type']=='docked_bike']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(docked_bike_df['ride_length2'])\n",
    "\n",
    "plt.xlabel('No.of Match Played')\n",
    "plt.ylabel('ride_length')\n",
    "plt.title('Histogram of Matches Played')\n",
    "\n",
    "# sns.heatmap(docked_bike_df['ride_length2'], annot=True, cmap='YlGnBu', fmt='.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docked_bike_ride_len = docked_bike_df.pivot_table(values='ride_length2',index=('member_casual'),aggfunc='max')\n",
    "docked_bike_ride_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1357/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze that causal customer used docked bike for range of 0 to 600 minutes.We also have a causual docked bike ride of approx. 23 hours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "From this analysis we found out that casual customer likely to use electric bike more and they take trips more on weekends with ride length of average 25 minutes.We also found that docked bike are used by casual customers only.\n",
    "Member customer not used docked bike and they take trip more on weekdays with average ride length of 17 minutes.We also see that there is no favourite bike in case of member customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3 recommendation for digitally attracting more casual riders to be members are:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Advertisement on screen of electric bike/offers on ride with electric bike.\n",
    "### - Offers on weekends for casual riders if they become members.\n",
    "### - Promoting docked_bike in member plan."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
